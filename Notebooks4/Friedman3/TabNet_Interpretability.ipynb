{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability\n",
    "\n",
    "Interpretability is the extent to which a cause and effect can be observed in a system, and the model can be understood in human terms.  Interpretability allows us to understand what exactly a model is learning, what other information the model has to offer, and the justifications behind its decisions, and evaluate all of these in the context of the real-world problem we are trying to solve.\n",
    "TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability. Feature selection is instance-wise, e.g. it can be different for each row of the training dataset.TabNet enables two kinds of interpretability: local interpretability that visualizes the importance of features and how they are combined for a single row, and global interpretability which quantifies the contribution of each feature to the trained model across the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will look at the interpretability of TabNet using the Friedman3 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "sys.path.insert(0, '../../src')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "import friedman3Dataset\n",
    "import dataset \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4\n",
    "n_samples= 100\n",
    "n_target = 1\n",
    "X,Y = friedman3Dataset.friedman3_data(n_samples)\n",
    "\n",
    "# Train test split for dataset \n",
    "real_dataset = dataset.CustomDataset(X,Y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "y_train = np.reshape(y_train, (-1, 1))\n",
    "y_test = np.reshape(y_test, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs =1000\n",
    "batch_size = 32\n",
    "\n",
    "regressor = TabNetRegressor(optimizer_fn=torch.optim.Adam, \n",
    "                        optimizer_params=dict(lr = 0.001),\n",
    "                        mask_type= 'sparsemax',\n",
    "                        verbose = 1)  \n",
    "\n",
    "regressor.fit(X_train = X_train,y_train = y_train, \n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        eval_name=['train', 'valid'], \n",
    "        eval_metric=[ 'mae'], \n",
    "        max_epochs = n_epochs, \n",
    "        batch_size = batch_size,\n",
    "        patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainability_matrix , masks = regressor.explain(X_test)\n",
    "\n",
    "# Normalize the importance by sample\n",
    "normalized_explain_mat = np.divide(explainability_matrix, explainability_matrix.sum(axis=1).reshape(-1, 1)+1e-8)\n",
    "\n",
    "# Add prediction to better understand correlation between features and predictions\n",
    "val_preds = regressor.predict(X_test)\n",
    "\n",
    "# explain_and_preds = np.hstack([normalized_explain_mat, val_preds.reshape(-1, 1)])\n",
    "explain_and_preds = np.hstack([normalized_explain_mat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the majority of estimators, TabNet provides access to a ranking of features in terms of their overall importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = regressor.feature_importances_\n",
    "indices = np.argsort(feat_importances)\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "plt.title(\"Overall feature importances\")\n",
    "plt.barh(range(len(feat_importances)), feat_importances[indices],color=\"b\", align=\"center\")\n",
    "features = ['feature_{}'.format(i) for i in range(0, 4)]\n",
    "plt.yticks(range(len(feat_importances)), [features[idx] for idx in indices])\n",
    "# all features\n",
    "# plt.ylim([-1, len(feat_importances)])\n",
    "# Top 25 features\n",
    "plt.ylim([len(feat_importances)-5, len(feat_importances)])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the beauty of TabNet is that it allows us to not only to obtain the overall feature importances, but also inspect the importance of each of the features for each of the individual rows, here for the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(explain_and_preds[:,:],\n",
    "          labels=dict(x=\"Features\", y=\"Samples\", color=\"Importance\"),\n",
    "          #x=features+[\"prediction\"],\n",
    "          title=\"Sample wise feature importance\",\n",
    "          color_continuous_scale='Jet',\n",
    "          height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see the variation of certain feature importances along the rows. This explains more about how each feature behaves throughout the dataset, which cannot be brought out by the simple overall ranking of the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also produce a correlation matrix for the importance of the features with respect to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_and_preds = np.hstack([normalized_explain_mat, val_preds.reshape(-1, 1)])\n",
    "correlation_importance = np.corrcoef(explain_and_preds.T)\n",
    "px.imshow(correlation_importance,\n",
    "          labels=dict(x=\"Features\", y=\"Features\", color=\"Correlation\"),\n",
    "          x=features+[\"prediction\"], y=features+[\"prediction\"],\n",
    "          title=\"Correlation between attention mechanism for each feature and predictions\",\n",
    "          color_continuous_scale='Jet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6eb94031926aa2cdb87e0870b3dc6626e89cd2ea57e62898b7b6ca21075da6b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
